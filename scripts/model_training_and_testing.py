# -*- coding: utf-8 -*-
"""model-training-and-testing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wledOFeWGCehYE4UlgpV74-cedByUylC
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from imblearn.over_sampling import SMOTE
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

cleaned_df = pd.read_csv("/content/cleaned_data.csv")

cleaned_df.head()

correlation = cleaned_df.select_dtypes('number').corr()
sns.heatmap(correlation)

# drop columns of effect of  accident
x_columns = cleaned_df.drop(columns =  ['Type_of_collision','Casualty_class', 'Number_of_casualties', 'Number_of_vehicles_involved',
       'Sex_of_casualty', 'Age_band_of_casualty', 'Casualty_severity',
       'Work_of_casuality', 'Fitness_of_casuality', 'Pedestrian_movement',
       'Cause_of_accident', 'Accident_severity'])

X = x_columns
y = cleaned_df['Accident_severity']

# Convert categorical variables into numeric using Label Encoding
label_encoders = {}
for column in X.columns:
    if X[column].dtype == 'object':
        le = LabelEncoder()
        X[column] = le.fit_transform(X[column])
        label_encoders[column] = le  # Save the encoder for later use

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initialize SMOTE
smote = SMOTE(random_state=42)

# Apply SMOTE to the training data
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)
# Initialize LabelEncoder
le = LabelEncoder()

# Fit and transform the labels in the training set
y_train_encoded = le.fit_transform(y_train)

# Transform the labels in the test set
y_test_encoded = le.transform(y_test)

# If you are using resampled data from SMOTE
y_train_resampled_encoded = le.fit_transform(y_train_resampled)

# Initialize XGBoost
xgb_model = XGBClassifier(random_state=42)

# Train the model
xgb_model.fit(X_train_resampled, y_train_resampled_encoded)

# Predict and evaluate
y_pred = xgb_model.predict(X_test)

xgb_model = XGBClassifier(random_state=42)

# Train the model using encoded labels
xgb_model.fit(X_train_resampled, y_train_resampled_encoded)

# Predict and evaluate
y_pred = xgb_model.predict(X_test)

# Inverse transform the predictions back to original labels for evaluation
y_pred_original = le.inverse_transform(y_pred)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred_original)
print("Accuracy:", accuracy)
print("\nClassification Report:\n", classification_report(y_test, y_pred_original))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred_original))

# Create a DataFrame for the actual and predicted values
predictions_df = pd.DataFrame({
    'Actual': y_test,
    'Predicted': y_pred_original
})

# Display the first few rows of the DataFrame
print(predictions_df.head(10))

predictions_df['Predicted'].nunique()

importances = rf_model.feature_importances_
indices = np.argsort(importances)[::-1]

# Plot the feature importance
plt.figure(figsize=(12, 6))
plt.title("Feature Importance")
plt.bar(range(X_train.shape[1]), importances[indices], align="center")
plt.xticks(range(X_train.shape[1]), X_train.columns[indices], rotation=90)
plt.tight_layout()
plt.show()

import pickle

# Save your trained model
model_filename = 'rf_model.pkl'
with open(model_filename, 'wb') as model_file:
    pickle.dump(rf_model, model_file)